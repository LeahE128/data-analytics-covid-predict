{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "detected-genealogy",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import relevant modules\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "from patsy import dmatrices\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.tree import export_graphviz\n",
    "\n",
    "import graphviz\n",
    "from graphviz import Source\n",
    "\n",
    "# ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caroline-concentration",
   "metadata": {},
   "source": [
    "# (1) Data understanding and prep:\n",
    "\n",
    "The original dataset has been cleaned (Homework1) and is now imported as a starting point for this work. \n",
    "1. The accompanying data quality report from Task1 can be found attached as PDF as a background to this cleaned dataset;\n",
    "2. A summary of this plan can be seen in the table below;\n",
    "\n",
    "\n",
    "# Summary of data quality plan from Assignment 1:\n",
    "\n",
    "\n",
    "| Variable Names                     | Data Quality Issue                                 | Handling Strategy                                                                  |\n",
    "|------------------------------------|----------------------------------------------------|------------------------------------------------------------------------------------|\n",
    "| cdc_report_dt                      | Depreciated by CDC                                 | Dropped column in Part 1                                                           |\n",
    "| pos_spec_dt                        | 67.88% missing values                              | Drop column                                                                        |\n",
    "| cdc_case_earliest_dt               | NA                                                 | Do nothing                                                                         |\n",
    "| onset_dt                           | 44.97% missing values                              | Do nothing                                                                         |\n",
    "| sex                                | 0.062% missing values                              | Replace missing values with mode                                                   |\n",
    "| sex                                | 0.682% unknown values                              | Do nothing                                                                         |\n",
    "| age_group                          | 0.126% missing values                              | Replace missing values with mode                                                   |\n",
    "| race_ethnicity_combined            | 1.017% missing values                              | Replace missing values with mode                                                   |\n",
    "| race_ethnicity_combined            | 38.70% unknown values                              | Do nothing                                                                         |\n",
    "| hosp_yn                            | 22.64% missing values                              | Combine with unknown values                                                        |\n",
    "| hosp_yn                            | 16.56% unknown values                              | Combine with missing values                                                        |\n",
    "| hosp_yn                            | 2 outliers of 'OTHER'                              | Remove outliers                                                                    |\n",
    "| icu _yn                            | 75.34% missing values                              | Combine with unknown values                                                        |\n",
    "| icu _yn                            | 13.52% unknown values                              | Combine with missing values                                                        |\n",
    "| death_yn                           | NA                                                 | Do nothing                                                                         |\n",
    "| medcond_yn                         | 73.83% missing values                              | Combine with unknown values                                                        |\n",
    "| medcond_yn                         | 8.15% unknown values                               | Combine with missing values                                                        |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "constitutional-constant",
   "metadata": {},
   "source": [
    "##  Part 1 1.1 Review, prepare and split the dataset into two datasets: 70% training and 30% test\n",
    "Here we will import cleaned dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "protected-trout",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read csv file into a dataframe.\n",
    "df = pd.read_csv('covid19-cdc-13336431-cleaned_data_Final3.csv', keep_default_na=True, sep=',\\s+', delimiter=',', skipinitialspace=True)\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pregnant-period",
   "metadata": {},
   "source": [
    "After importing the cleaned csv, as a reminder we will check the shape, inspect the datatypes and check for any remaining null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unavailable-newcastle",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "blind-center",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tamil-portal",
   "metadata": {},
   "source": [
    "#### Convert datatypes for plotting later\n",
    "We will now review the datatypes and convert if needed. This will help avoid plotting errors later in the notebook\n",
    "- The target feature \"death_yn\" is type object, with values \"Yes\" & \"No\". These will be mapped 'yes': 1, \"no\": 0 and stored as \"int64\"\n",
    "- Categorical features will stay as category datatypes\n",
    "- Some Continuous features are datetime64 type\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "monthly-ambassador",
   "metadata": {},
   "source": [
    "#### Setup Column types\n",
    "We will now setup the continuous, categorical, target features\n",
    "\n",
    "##### Select all categorical columns and convert to categorical type\n",
    " - This will be needed later when it will be required to convert categorical features into dummy features for modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "operating-mixture",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert riskperformance to 0,1\n",
    "df['death_yn'] = df['death_yn'].map({'Yes': 1, \"No\": 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "agricultural-circuit",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert date time features to appropriate date time data types\n",
    "df['cdc_case_earliest_dt'] = df['cdc_case_earliest_dt'].astype('datetime64[ns]')\n",
    "df.dtypes\n",
    "\n",
    "#Select all columns of type 'object'\n",
    "object_columns = df.select_dtypes(['object']).columns\n",
    "object_columns\n",
    "#Convert selected columns to type 'category'\n",
    "for column in object_columns:\n",
    "    df[column] = df[column].astype('category')\n",
    "df.dtypes \n",
    "continous_columns = df.select_dtypes(['datetime64[ns]']).columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "incorrect-consequence",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "posted-withdrawal",
   "metadata": {},
   "source": [
    "##### Finally set the target feature \"death_yn\" to int64 \n",
    "- death_yn is a categorical feature but it is also the target feature\n",
    "- To allow continuous features to plot against the target, it will need to be int64 type\n",
    "- We will also remove it from the list of categorical columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "earned-pollution",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['death_yn'] = df['death_yn'].astype('int64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unavailable-classic",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_columns = df[[\"current_status\",\"sex\",\"age_group\",\"Race\",\"Ethnicity\", \"hosp_yn\", \"icu_yn\",\"medcond_yn\"]].columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecological-design",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "directed-lottery",
   "metadata": {},
   "source": [
    "##### The dataset will now be split into two datasets: 70% training and 30% test\n",
    "- First we will set the target feature \"y\" to be risk performance\n",
    "- Then we will set \"X\" to be the remaining features in the dataframe i.e. we drop \"death_yn\" from the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "behind-grounds",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = pd.DataFrame(df[\"death_yn\"])\n",
    "X = df.drop([\"death_yn\"],1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "thermal-salmon",
   "metadata": {},
   "source": [
    "##### The data set can now be split\n",
    "- The train test split will randomly split the dataset as per the test size\n",
    "- We will set the random state=1 to allow the random shuffle to be repeated within this notebook only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "informal-killing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into two datasets: 70% training and 30% test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3,random_state=1)\n",
    "\n",
    "print(\"original range is: \",df.shape[0])\n",
    "print(\"training range (70%):\\t rows 0 to\", round(X_train.shape[0]))\n",
    "print(\"test range (30%): \\t rows\", round(X_train.shape[0]), \"to\", round(X_train.shape[0]) + X_test.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mobile-malpractice",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.dtypes\n",
    "y_train.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "operating-romance",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "informative-velvet",
   "metadata": {},
   "source": [
    "## Part 1 1.2 On the training set we will now carry out a series of plots comparing all features to help make decisions on what features to keep for the model \n",
    "\n",
    "- All plots will be using the training subset of dataset: X_train, y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alpine-produce",
   "metadata": {},
   "source": [
    "### 1.2 Plot interaction between categorical features and target feature\n",
    "- Here we will plot a pairwise interaction between each categorical feature against the target feature. \n",
    "- We will discuss what we observe from these plots, e.g., which categorical features seem to be better at predicting the target feature...\n",
    "- We will choose a subset of categorical features we find promising (if any) and justify our choice.\n",
    "\n",
    "Here we have 8 categorical features\n",
    "- current_status, sex, age_group, hosp_yn, icu_yn, medcond_yn, race, ethnicity \n",
    "- A stacked bar plot for each will be created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "understanding-bedroom",
   "metadata": {},
   "outputs": [],
   "source": [
    "## for the one continous feature\n",
    "##create plot for cdc_earliest_case\n",
    "date_df = df.groupby([\"cdc_case_earliest_dt\", \"death_yn\"])[\"death_yn\"].count().unstack(\"death_yn\").fillna(0)\n",
    "date_df\n",
    "\n",
    "date_df.plot(kind=\"barh\", figsize=(10, 80), stacked=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "expensive-homeless",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Create stacked bar plots for all categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "functional-event",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.patches as mpatches\n",
    "# for the comparison we will convert the target back to categorical\n",
    "y_train = y_train.astype(\"category\")\n",
    "#X_train[categorical_columns].columns\n",
    "\n",
    "# we will create temp dataframe for these plots, mergeing X_train and y_train\n",
    "df_temp= pd.concat([X_train, y_train], axis=1)\n",
    "\n",
    "\n",
    "# for each categorical feature create a stacked bar plot\n",
    "for categorical_feature in categorical_columns:\n",
    "    # Using code from the module lab\n",
    "    \n",
    "    maxDelqEver = pd.unique(df_temp[categorical_feature].ravel())\n",
    "\n",
    "    # add new column and set values to zero\n",
    "    df_temp['percent'] = 0\n",
    "\n",
    "    #print header\n",
    "    print(\"\\n\", categorical_feature)\n",
    "    print(\"Index \\t Count\")\n",
    "\n",
    "    # for each delinquency category\n",
    "    for i in maxDelqEver:\n",
    "\n",
    "        count = df_temp[df_temp[categorical_feature] == i].count()['death_yn']\n",
    "        count_percentage = (1 / count) * 100\n",
    "\n",
    "        # print out index vs count\n",
    "        print(i, \"\\t\", count)\n",
    "\n",
    "        index_list = df_temp[df_temp[categorical_feature] == i].index.tolist()\n",
    "        for ind in index_list:\n",
    "            df_temp.loc[ind, 'percent'] = count_percentage\n",
    "\n",
    "    group = df_temp[['percent',categorical_feature,'death_yn']].groupby([categorical_feature,'death_yn']).sum()\n",
    "\n",
    "    my_plot = group.unstack().plot(kind='bar', stacked=True, title=f\"Death_yn vs {categorical_feature}\", figsize=(15,7), grid=True)\n",
    "\n",
    "    # add legend\n",
    "    red_patch = mpatches.Patch(color='orange', label='Yes')\n",
    "    blue_patch = mpatches.Patch(color='blue', label='No')\n",
    "    my_plot.legend(handles=[red_patch, blue_patch], frameon = True)\n",
    "\n",
    "    # add gridlines\n",
    "    plt.grid(b=True, which='major', color='#666666', linestyle='-')\n",
    "    plt.minorticks_on()\n",
    "    plt.grid(b=True, which='minor', color='#999999', linestyle='-', alpha=0.2)\n",
    "\n",
    "    # add labels\n",
    "    my_plot.set_xlabel(\"Feature Values\")\n",
    "    my_plot.set_ylabel(\"% of rows\")\n",
    "    my_plot.set_ylim([0,100])\n",
    "    \n",
    "# drop 'percent' that was used only for stacked bar plot\n",
    "df_temp = df_temp.drop(\"percent\", 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "affiliated-decade",
   "metadata": {},
   "source": [
    "#### Interpretation of the results\n",
    "#### death_yn vs cdc_earliest_case_dt\n",
    "- Observing the plots we can see there is no significant relationship between the cdc_earliest_case_dt of a COVID-19 positive case and death.\n",
    "\n",
    "##### death_yn vs current_status\n",
    "- Observing the plots we can see there is no significant relationship between the current_status of a COVID-19 positive case and death. \n",
    "- Both status' have a similar % probability of death and both are not significantly high ~ less than 5%.\n",
    "- Due to this it would seem that current_status does not have a significant effect on likelihood of death \n",
    "- It is not a significant high value feature.\n",
    "\n",
    "##### death_yn vs sex\n",
    "- Observing the plots we can see there is no significant relationship between the sex of COVID-19 positive cases and death. \n",
    "- Both male and female genders have a similar % probability of death and both are not significantly high ~ less than 5%and unknown sex is ~ 2%, suggesting sex does not significantly effect the liklihood of death from COVID-19.\n",
    "- It is not a significant high value feature.\n",
    "\n",
    "##### death_yn vs age_group\n",
    "- Observing the plots we can see there is a significant relationship between the age of COVID-19 positive cases and death. \n",
    "- The plot shows a significant relationship between older age groups and death, it seems the risk of death increases as one gets older, with the 80+ year olds being most at risk of death when COVID-19 positive with ~ 35% of 80+ year olds dying of COVID-19.\n",
    "- The younger age groups show much less risk of dying from COVID-19 when COVID-19 positive.\n",
    "- age_group is a significant high value feature. \n",
    "\n",
    "##### death_yn vs race\n",
    "- Observing the plots we can see there is no significant relationship between the race of COVID-19 positive cases and death. \n",
    "- The highest % deaths are in the white race category ~ 5% but this may be due to white race having the highest number of cases, as a result all races could have proportional numbers of death relative to number of COVID-19 positive cases.\n",
    "- It is not a significant high value feature.\n",
    "\n",
    "##### death_yn vs hosp_yn\n",
    "- Observing the plots we can see there is a significant relationship between hosipitalisations of COVID-19 positive cases and death. \n",
    "- The plot shows a significant relationship between hospitalised as a result of COVID-19 and death, it seems the risk of death increases if a positive case is hospitalised with ~30% of positive casees who were hospitalised dying as a result of COVID-19.\n",
    "- hosp_yn is a significant high value feature.\n",
    "\n",
    "##### death_yn vs icu_yn\n",
    "- Observing the plots we can see there is a significant relationship between positive cases being admitted to ICU  and death. \n",
    "- The plot shows a significant relationship between icu admittance as a result of COVID-19 and death, it seems the risk of death increases if a positive case is admitted to ICU with ~60% of positive casees who were admitted to icu dying as a result of COVID-19.\n",
    "- icu_yn is a significant high value feature.\n",
    "\n",
    "##### death_yn vs medcond_yn\n",
    "- Observing the plots we can see there is a significant relationship between positive cases having existing medical conditions and death. \n",
    "- The plot shows a significant relationship between having an exisiting medical condition and being COVID-19 positive and death, it seems the risk of death increases if a positive case has a medical condition with ~12% of positive casees who had a medical condition dying as a result of COVID-19.\n",
    "- Although, the risk of death and having a medical condition is not as high as being admitted to icu, having a medical condition does seem to increase the risk of death and therefore, medcond_yn is a high value feature.\n",
    "\n",
    "\n",
    "**Categorical features will be split into low value features and high value features, low value features include current_status, sex, race and ethnicity and will be dropped, high value features include age_group, hosp_yn, icu_yn, medcond_yn and these features will be kept**\n",
    "\n",
    "**cdc_case_earliest_dt is the only continuous feature and will also be dropped due to low correlation with target feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fluid-carnival",
   "metadata": {},
   "outputs": [],
   "source": [
    "low_gain_features = ['current_status', 'sex', 'Race', 'Ethnicity']\n",
    "low_correlation_features = ['cdc_case_earliest_dt']\n",
    "# drop all low value features\n",
    "low_value_features = list(set(low_gain_features + low_correlation_features))\n",
    "print(low_value_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "changing-delhi",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alternate-pricing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop all low value features\n",
    "# before dropping make copy of original\n",
    "df_rev1 = df.copy()\n",
    "# drop low value features\n",
    "df_rev1.drop(low_value_features, 1, inplace=True)\n",
    "print('\\nRemaining columns:', df_rev1.columns)\n",
    "print('\\nNew shape:', df_rev1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unique-heading",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rev1.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bacterial-expert",
   "metadata": {},
   "source": [
    "## Part 1 1.2 Prepare dataset for modeling\n",
    "Now we have picked our descriptive features for the whole dataset, a number of additional steps will need to be taken to prepare the dataset for modeling \n",
    "- We will now convert the categorical variables into dummies variable to allow modeling\n",
    "- We will then set up the train test split again based on the dataset with the dummies included"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "false-texture",
   "metadata": {},
   "source": [
    "#### Set up dummy features. \n",
    "This will split up each categorical feature into a number of dummy features. The data type changes to \"uint8\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "disabled-madison",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rev1 = pd.get_dummies(df_rev1, columns=['age_group','hosp_yn','icu_yn','medcond_yn'], drop_first=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "average-bishop",
   "metadata": {},
   "source": [
    "#### Categorical columns are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bigger-portal",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_columns = df_rev1.select_dtypes(include=['uint8']).columns.tolist()\n",
    "\n",
    "\n",
    "categorical_columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "absent-patrick",
   "metadata": {},
   "source": [
    "### Setting up the train/test split\n",
    "- This is based on the dataset with dummy values\n",
    "- The Target is stored in dataframe \"y\"\n",
    "- The remaining features are stored in dataframe \"X\"\n",
    "- Both are split into training and test subsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dramatic-retail",
   "metadata": {},
   "outputs": [],
   "source": [
    "# y is the target\n",
    "y = df_rev1[\"death_yn\"]\n",
    "\n",
    "\n",
    "# X is everything else\n",
    "X = df_rev1.drop([\"death_yn\"],1)\n",
    "# Split the dataset into two datasets: 70% training and 30% test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1,  test_size=0.3)\n",
    "\n",
    "print(\"original range is: \",df_rev1.shape[0])\n",
    "print(\"training range (70%):\\t rows 0 to\", round(X_train.shape[0]))\n",
    "print(\"test range (30%): \\t rows\", round(X_train.shape[0]), \"to\", round(X_train.shape[0]) + X_test.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "metric-morgan",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nDescriptive features in X:\\n\", X_train.head(5))\n",
    "print(\"\\nTarget feature in y:\\n\", y_train.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "metallic-aaron",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "advisory-chest",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train\n",
    "y_train.head(5)\n",
    "y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "literary-yellow",
   "metadata": {},
   "source": [
    "#### Reset the indexes of the training and test splits\n",
    "- We can see from the X_train printout below that the indexes are no longer consecutive\n",
    "- This is the same for y_train, X_test, y_test\n",
    "- This will need to be dealt with next as it will effect merging of dataframes in the coming sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "european-spanish",
   "metadata": {},
   "outputs": [],
   "source": [
    "# need to reset the index to allow contatenation with predicted values otherwise not joining on same index...\n",
    "X_train.reset_index(drop=True, inplace=True)\n",
    "y_train.reset_index(drop=True, inplace=True)\n",
    "X_test.reset_index(drop=True, inplace=True)\n",
    "y_test.reset_index(drop=True, inplace=True)\n",
    "X_train.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "concrete-pleasure",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "opened-pricing",
   "metadata": {},
   "source": [
    "The datasets are now ready for modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "southeast-palmer",
   "metadata": {},
   "source": [
    "## Part 2 Linear Regression Model\n",
    "#### 2.1 Train a linear regression model using only the descriptive features selected from part 1 above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "french-parking",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train aka fit, a model using all continuous and categorical features.\n",
    "multiple_linreg = LinearRegression().fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "promising-examination",
   "metadata": {},
   "source": [
    "#### 2.2 Print the coefficients learned by the model and discuss their role in the model (e.g., interpret the model by analysing each coefficient and how it relates each input feature to the target feature)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "qualified-chicken",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the weights learned for each feature.\n",
    "print(\"\\nFeatures are: \\n\", X_train.columns)\n",
    "print(\"\\nCoeficients are: \\n\", multiple_linreg.coef_)\n",
    "print(\"\\nIntercept is: \\n\", multiple_linreg.intercept_)\n",
    "print(\"\\nFeatures and coeficients: \\n\", list(zip(X_train.columns, multiple_linreg.coef_)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hidden-checkout",
   "metadata": {},
   "source": [
    "#### Correlation coefficients are used to measure how strong a relationship is between two variables.\n",
    "\n",
    "Correlation coefficient formulas are used to find how strong a relationship is between data. The formulas return a value between -1 and 1, where:\n",
    "\n",
    "1 indicates a strong positive relationship.\n",
    "-1 indicates a strong negative relationship.\n",
    "A result of zero indicates no relationship at all.\n",
    "\n",
    "In this case most features show a weak positive relationship between 0 and 0.1. The features that have the weakest relationship with the target feature is 'age_group_30 - 39 Years', -0.001203725227328168,'medcond_yn_unknown', -0.007032338248988288,'age_group_30 - 39 Years', -0.001203725227328168 which all have negative correlation co-efficients and the features with the strongest relationship with the target feature is age_group_80+ Years', 0.27613110357796317,  'hosp_yn_Yes', 0.16592315710540517 and 'icu_yn_Yes', 0.4040060481753917."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "valuable-monaco",
   "metadata": {},
   "source": [
    "#### 2.2 Interpreting the linear regression model\n",
    "\n",
    "- Linear regression is a modeling tool that is used to make predictions based on linear relationship between the target (dependent variable) and any number of predictors (independent variables)\n",
    "    - It finds the line of best fit the describes the relationship between the target and predictors \n",
    "    - This line is calculated by minimising the overall error\n",
    "- The purpose of regression analysis is to:\n",
    "    - Predict the value of the dependent variable as a function of the value(s) of at least one independent variable.\n",
    "    - Explain how changes in an independent variable are manifested in the dependent variable\n",
    "    - The dependent variable is the variable that is to be predicted or explained.\n",
    "    - An independent variable is the variable or variables that is used to predict or explain the dependent variable\n",
    "- The linear regression formula takes the following form:\n",
    "    - $target\\_feature = w_0 + w_1 * feature_1 + w_2*feature_2 + ...+ w_n*feature_n $\n",
    "    - The output of this formula will be a continuous value that can be less than 0 and higher than 1\n",
    "- We can see the calculated intercept is -0.030 (w_0)\n",
    "    - This is the starting point. i.e. if all other coefficients are zero then the model result will be -0.030\n",
    "    - Can be thought of as where the model line intercepts the y axis\n",
    "- We can see all the coefficients for each feature\n",
    "    - These are zipped together in a single list for ease of inspection.\n",
    "    - The sum of all the feature*coefficients + intercept will result in the model prediction  \n",
    "- We cannot make a direct comparison based on the value of the coefficients as it is tied directly to the range of each feature.\n",
    "    - If all features were normalized this would be possible to see directly\n",
    "    - However when categorical features are one-hot encoded, the values are either 1 or 0, so the scaling range is not as much a problem as values fall within either 0 or 1, meaning normalisation is not needed. \n",
    "    - Normalization/standardization of features is done to bring all features to a similar scale. When you one hot encode categorical variables they are either 0/1 hence there is not much scale difference\n",
    "    \n",
    "It is important to note that the output from a linear regression model is not suited to the classification problem that we are trying to solve.\n",
    "- The output is not a probability and an additional thresholding step is necessary to convert the output into a binary classification\n",
    "- We will threshold the output so that any values >=0.5 will be cast to 1, any values <0.5 will be cast to 0\n",
    "\n",
    "Finally it is worth mentioning the effect outliers can have on linear regression output. \n",
    "- If an extra training example is included and it is an outlier,the outlier will have a huge effect on values near the threshold. \n",
    "- This sensitivity to outliers is one reason linear regression is a poor choice for classification problems.\n",
    "- Additionally, another reason that linear regression is a poor choice for classification problems is that linear regression models output values that are continuous and can be far greater than 1 and far less than 0. Since our classes are discrete, only consisting of 0 and 1 i.e death yes or no, linear regression does not seem a plausible solution to this problem.\n",
    "\n",
    "** Got information from DA lab solution.\n",
    "\n",
    "source: https://medium.com/@rgotesman1/learning-machine-learning-part-3-logistic-regression-94db47a94ea3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afraid-boulder",
   "metadata": {},
   "source": [
    "## Part 2 2.3 Printing 10 predicted target features and evaluate the prediction\n",
    "Print the predicted target feature value for the first 10 training examples. Threshold the predicted target feature value given by the linear regression model at 0.5, to get the predicted class for each example. Print the predicted class for the first 10 examples. Print a few classification evaluation measures computed on the full training set (e.g., Accuracy, Confusion matrix, Precision, Recall, F1) and discuss your findings so far."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tutorial-yield",
   "metadata": {},
   "source": [
    "\n",
    "- Here we will print the predicted target feature value for the first 10 training examples. \n",
    "- We will threshold the predicted target feature value given by the linear regression model at 0.5, to get the predicted class for each example. \n",
    "    - If value is >= 0.5 it is cast to 1, if < 0.5 it is cast to 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "champion-double",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the prediction and threshold the value. If >= 0.5 its true\n",
    "multiple_linreg_predictions_train = (multiple_linreg.predict(X_train) >= 0.5) * 1.0\n",
    "\n",
    "print(\"\\nPredictions with multiple linear regression: \\n\")\n",
    "actual_vs_predicted_multiplelinreg = pd.concat([y_train, pd.DataFrame(multiple_linreg_predictions_train, columns=['Predicted'])], axis=1)\n",
    "print(actual_vs_predicted_multiplelinreg.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cardiovascular-mitchell",
   "metadata": {},
   "source": [
    "## Part 2 2.4 Evaluation metrics based on training data\n",
    "- We will print the classification evaluation measures computed on the training set (e.g. Accuracy, Confusion matrix, Precision, Recall, F1)\n",
    "- We will discuss findings based on these measures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "attempted-england",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some more evaluation metrics.\n",
    "print(\"==================== Train Data =======================\")\n",
    "print(\"Accuracy: \", metrics.accuracy_score(y_train, multiple_linreg_predictions_train))\n",
    "print(\"Confusion matrix: \\n\", metrics.confusion_matrix(y_train, multiple_linreg_predictions_train))\n",
    "print(\"Classification report:\\n \", metrics.classification_report(y_train, multiple_linreg_predictions_train))\n",
    "print(\"======================================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "narrow-irish",
   "metadata": {},
   "source": [
    "## Interpretation of results\n",
    "- Accuracy\n",
    "    - This is simply stating how often the model is correct. We have an accuracy of 96.6%\n",
    "- Confusion Matrix - A confusion matrix is a table that is often used to describe the performance of a classification model (or \"classifier\") on a set of test data for which the true values are known.\n",
    "    - [0][0] TRUE NEGATIVE - The number that is predicted 0 that was actually 0\n",
    "    - [0][1] FALSE POSITIVE - The number that is predicted 1 that is actually 0\n",
    "    - [1][0] FALSE NEGATIVE - The number that is predicted 0 that is actually 1\n",
    "    - [1][1] TRUE POSITIVE - The number that is predicted 1 that is actually 1\n",
    "- Precision - How good the model is at predicting the positive class\n",
    "    - What % of the predicted positive are actually positive\n",
    "    - It is the number values correctly predicted positive over the total number of  positive values\n",
    "    - Precision Positive is 0.62\n",
    "    - Precision Negative is 0.97\n",
    "- Recall - The precise definition of recall is the number of true positives divided by the number of true positives plus the number of false negatives. \n",
    "    - What % of the positive values did we predict\n",
    "    - Is the number correctly predicted positive over the total number actual positive\n",
    "    - Recall Positive is 0.16\n",
    "    - Recall Negative is 1.00\n",
    "- F1 Score\n",
    "    - Is an a weighted average of Precision and recall\n",
    "    - F1 Score Positive is 0.26\n",
    "    - F1 Score Negative is 0.98\n",
    "\n",
    "Summary\n",
    "- Theses values seem reasonable. \n",
    "- The model is a much better at predicting the negative class i.e not death.\n",
    "- This makes sense as the model has significantly more negative data to learn from - it is the majority class. \n",
    "- This is not ideal as it does not suit the need to determine features that may increase risk of death as a result of COVID-19. \n",
    "- This represents an imbalanced classification problem: we have two classes we need to identify — death and not death — with one category representing the overwhelming majority of the data points i.e not death.\n",
    "- This is not ideal as the model may underestimate the number of deaths as a result of COVID-19.\n",
    "- These types of problems are examples of when accuracy is not a good measure for assessing model performance because essentially what the model has done is cleverly realised that if it predicts no deaths for the majority of cases, because the data set is imbalanced to the non-death side, it most likely is accurate in predicting non-deaths only because that's what the majority of the data is but the model will not correctly identify the risks that affect the outcome and as such cannot accurately predict factors related to COVID-19 deaths and that's why the recall value is so low and the positive precision value is 62%.\n",
    "- The important statistic to look at here is the recall values, the recall value of determining true positives is 16%.\n",
    "- This essentially means the model is very poor at predicting deaths as there are a lot of false negatives.\n",
    "- The down side of this is more people who have a risk factor may die as a result of COVID-19 because the model understimated their liklihood of dying due to a COVID-19 related risk.\n",
    "- This would not be an appropriate model to use to determine risk factors of COVID-19 deaths as a result."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "regional-sally",
   "metadata": {},
   "source": [
    "## Part 2 2.4 Evaluate the model using the hold-out (30% examples) test set\n",
    "- The results from the test data will be compared the results from the training data.\n",
    "- In addition they will be compared to the results from a cross-validated model (i.e. a new model trained and evaluated using cross-validation on the full dataset)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lonely-rough",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the prediction and threshold the value. If >= 0.5 its true\n",
    "multiple_linreg_predictions_test = (multiple_linreg.predict(X_test) >= 0.5) * 1.0\n",
    "\n",
    "print(\"\\nPredictions with multiple linear regression: \\n\")\n",
    "actual_vs_predicted_multiplelinreg = pd.concat([y_test, pd.DataFrame(multiple_linreg_predictions_test, columns=['Predicted'])], axis=1)\n",
    "print(actual_vs_predicted_multiplelinreg.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "greek-trinidad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some more evaluation metrics.\n",
    "print(\"==================== Test Data =======================\")\n",
    "print(\"Accuracy: \", metrics.accuracy_score(y_test, multiple_linreg_predictions_test))\n",
    "print(\"Confusion matrix: \\n\", metrics.confusion_matrix(y_test, multiple_linreg_predictions_test))\n",
    "print(\"Classification report - Test data:\\n \", metrics.classification_report(y_test, multiple_linreg_predictions_test))\n",
    "print(\"\\n==================== Train Data ======================\")\n",
    "print(\"Accuracy: \", metrics.accuracy_score(y_train, multiple_linreg_predictions_train))\n",
    "print(\"Confusion matrix: \\n\", metrics.confusion_matrix(y_train, multiple_linreg_predictions_train))\n",
    "print(\"\\nClassification report: - Training data\\n \", metrics.classification_report(y_train, multiple_linreg_predictions_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "focused-tuning",
   "metadata": {},
   "source": [
    "**Interpretation of test results and comparison with training results**\n",
    "- Accuracy\n",
    "    - The accuracy of the test data is marginally lower 96.4% vs 96.6%. \n",
    "- Precision\n",
    "    - The precision score for predicting the positive case decreased from 62% to 45%.\n",
    "    - The precision score for predicting the negative case did not change as it was 97% in both cases.\n",
    "- Recall\n",
    "    - The recall score for predicting the positive case has decreased from 16% to 9%.\n",
    "    - The recall score for predicting the negative case did not change, it was 100% in both cases.\n",
    "- F1\n",
    "    - The f1 score for predicting the positive case has increased from 26% to 15%.\n",
    "    - The f1 score for predicting the negative case has reduced did not change, it was 98% in both cases.\n",
    "\n",
    "**Summary**\n",
    "- These values are what we expected as we are now testing the model prediction on data it has not seen before\n",
    " and the results prove this model is not a good model to use on real world data.\n",
    "- More comparisons need to be made and this is where cross validation steps in"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "directed-shelter",
   "metadata": {},
   "source": [
    "## Part 2 2.4 Cross validation\n",
    "\n",
    "Cross-validation is a statistical method used to estimate the skill of machine learning models.\n",
    "\n",
    "It is commonly used in applied machine learning to compare and select a model for a given predictive modeling problem because it is easy to understand, easy to implement, and results in skill estimates that generally have a lower bias than other methods.\n",
    "\n",
    "It is a validation method to see how accurate your model is on new data as an indicator of how it would perform on real-world data. \n",
    "\n",
    "k-fold cross validation is a procedure used to estimate the skill of the model on new data.\n",
    "\n",
    "\n",
    "- We will now perform cross validation on the linear regression model. \n",
    "- Here we perform the same evaluation as above but multiple times\n",
    "- Each time the data is shuffled so we get a slightly different view of the data for training and testing\n",
    "- This works well for evaluating on a limited set of data\n",
    "- We will store the results in a dictionary for later use\n",
    "\n",
    "* Got info from lab solution example.\n",
    "\n",
    "First we need to create a function to perform this cross validation. Sklearn does not provide one for linear regression. However it does for logistic and random forests models\n",
    "- Cross validation Function can be seen below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "major-victim",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_val_LinReg(X, y, cv=3, scoring='accuracy'):\n",
    "    \"\"\"Functions to carry out cross validation on the linear regression model\n",
    "    Default number of validations is 3. The randon state will be updated \n",
    "    at each iteration to allow our results to be repeated\"\"\"\n",
    "    \n",
    "    # store results\n",
    "    results = []\n",
    "    # evaluate cv times and append to results\n",
    "    for i in range(cv):\n",
    "        # set up train test split\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=i , test_size=0.3)\n",
    "        # generate model\n",
    "        multiple_linreg = LinearRegression().fit(X_train, y_train)\n",
    "        # threshold\n",
    "        multiple_linreg_predictions = (multiple_linreg.predict(X_test) >= 0.5) * 1.0\n",
    "        # calc score\n",
    "        if scoring=='accuracy':\n",
    "            score = metrics.accuracy_score(y_test, multiple_linreg_predictions)\n",
    "        elif scoring=='precision':\n",
    "            score = metrics.precision_score(y_test, multiple_linreg_predictions)\n",
    "        elif scoring=='f1':\n",
    "            score = metrics.f1_score(y_test, multiple_linreg_predictions)\n",
    "        elif scoring=='recall':\n",
    "            score = metrics.recall_score(y_test, multiple_linreg_predictions)\n",
    "        # append to results\n",
    "        results.append(score)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "occasional-lottery",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_val_LinReg_DF(X,y):\n",
    "    \"\"\"Function to perform cross validation and store results \n",
    "    in dataframe. Cross validation looks at accuracy, precision, \n",
    "    recall, f1. Returns a dataframe with results\"\"\"\n",
    "\n",
    "    linRegResults = {}\n",
    "    # metrics to test against\n",
    "    test_metrics = ['accuracy','precision','recall', 'f1']\n",
    "    \n",
    "    for metric in test_metrics:\n",
    "        # generate test results\n",
    "        result = cross_val_LinReg(X, y, cv=10, scoring=metric)\n",
    "        length = len(result)\n",
    "        # store result in dict\n",
    "        linRegResults[metric] = sum(result)/length\n",
    "\n",
    "    # create dataframe with results\n",
    "    LinRegDF = pd.DataFrame.from_dict(linRegResults, orient='index', columns=['Linear_Regression'])\n",
    "    \n",
    "    return LinRegDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compact-contact",
   "metadata": {},
   "outputs": [],
   "source": [
    "linRegDF = cross_val_LinReg_DF(X,y)\n",
    "print(f\"Mean results from 10 fold cross validation are:\")\n",
    "linRegDF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "motivated-teacher",
   "metadata": {},
   "source": [
    "These results are marginally lower than previous results but this is expected. We have taken the mean of 10 sets of results. This proved what was stated in the summary of interpretation of results, that this model would not accurately predict COVID-19 deaths based on certain risk factors and as such would not be a viable model to use on real-world data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "meaningful-rates",
   "metadata": {},
   "source": [
    "## Part 3 Logistic Regression  \n",
    "\n",
    "- Logistic regression is a statistical model that in its basic form uses a logistic function to model a binary dependent variable.\n",
    "\n",
    "- Linear regression is used to predict the continuous dependent variable using a given set of independent variables, whereas, Logistic Regression is used to predict the categorical dependent variable using a given set of independent variables so in theory should be better suited to this problem, where the output is a classification of death no or death yes.\n",
    "\n",
    "- In order to map predicted values to probabilities, a sigmoid function is used. The function maps any real value into another value between 0 and 1.\n",
    "\n",
    "    - The first part of logistic regression function is similar to linear regression i.e. We find the line of best fit\n",
    "    - We then pass this equation through what is called a sigmoid function\n",
    "    - This sigmoid function will output a value bound between 0 and 1. It is a probability\n",
    "    - The model then applies a threshold to this probability so that if is is >= 0.5 its cast to 1 and if it is <0.5 it is cast to 0\n",
    "    - All of these steps are carried out within the logistic regression function, however the threshold value can be adjusted up or down depending on the problem you are trying to solve.\n",
    "\n",
    "The model estimated in logistic regression is given by the logistic function: <br>\n",
    "$probability(target=1|descriptive\\_features)=logistic(w_0 + w_1 * feature_1 + w_2*feature_2 + ...+ w_n*feature_n)$ <br>\n",
    "where $logistic(x)$ is defined as: $logistic(x) = \\frac{e ^ x}{1 + e ^ x} = \\frac{1}{1+e^{-x}}$\n",
    "\n",
    "- From the values above can see the calculated intercept is -1.006\n",
    "    - This is the starting point. i.e. if all other coefficients were zero then the value for 'x' would be -1.006. \n",
    "    - This is the input to the logistic function and the logistic function will then calculate the probability and threshold based on this.\n",
    "- We can see all the coefficients for each features\n",
    "    - These are zipped together in a single list for ease of inspection.\n",
    "    - The effect of these on the value 'x' is the same as for linear regression\n",
    "    - Only when the value 'x' is fed into the logistic function do we see the real difference between the to methods.\n",
    "    \n",
    "A major benefit of logistic regression worth  highlighting is its ability to handle outliers. \n",
    "- As discussed for linear regression, outliers can significantly skew what values fall within each threshold point. \n",
    "- Outliers do not significantly effect the model in a logistic regression method.\n",
    "\n",
    "* Got information from DA Lab solution example.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Part 3 3.1 Train a logistic regression model using only the descriptive features selected from part 1 above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "committed-minister",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train aka fit, a model using all continuous and categorical features.\n",
    "multiple_logisticreg = LogisticRegression().fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unavailable-madison",
   "metadata": {},
   "source": [
    "## Part 3 3.2 Print the coefficients learned by the model and discuss their role in the model (e.g., interpret the model by analysing each coefficient and how it relates each input feature to the target feature)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "empirical-beatles",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the weights learned for each feature.\n",
    "print(\"\\nFeatures are: \\n\", X_train.columns)\n",
    "print(\"\\nCoeficients are: \\n\", multiple_logisticreg.coef_[0])\n",
    "print(\"\\nIntercept is: \\n\", multiple_logisticreg.intercept_)\n",
    "print(\"\\nFeatures and coeficients: \\n\", list(zip(X_train.columns, multiple_logisticreg.coef_[0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accepting-development",
   "metadata": {},
   "source": [
    "Correlation coefficients are used to measure how strong a relationship is between two variables.\n",
    "\n",
    "Correlation coefficient formulas are used to find how strong a relationship is between data. The formulas return a value between -1 and 1, where:\n",
    "\n",
    "1 indicates a strong positive relationship.\n",
    "-1 indicates a strong negative relationship.\n",
    "A result of zero indicates no relationship at all.\n",
    "\n",
    "In this case most features show a weak positive relationship between 0 and 0.3. The features that have the weakest relationship with the target feature is 'age_group_10 - 19 Years', -1.3177439596473888,'age_group_20 - 29 Years', -1.7009865673611022,'age_group_40 - 49 Years', -0.6451217529604351 which all have negative correlation co-efficients and the features with the strongest relationship with the target feature is 'age_group_80+ Years', 3.408052382571183, 'hosp_yn_Yes', 2.456620101265501, 'age_group_70 - 79 Years', 2.258483090701781 and 'icu_yn_Yes', 2.6308478366567143.\n",
    "\n",
    "These values have significantly increased in comparison to the correlation co-efficients determined using the linear regression model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "equivalent-needle",
   "metadata": {},
   "source": [
    "## Part 3 3.3 Evaluate the model using the hold-out (30% examples) test set\n",
    "- These results from the test data will be compared the results from the training data.\n",
    "- In addition they will be compared to the results from a cross-validated model (i.e., a new model trained and evaluated using cross-validation on the full dataset)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "simple-diagnosis",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the prediction and threshold the value. If >= 0.5 its true\n",
    "multiple_logisticreg_predictions_train = multiple_logisticreg.predict(X_train)\n",
    "\n",
    "print(\"\\nPredictions with multiple logistic regression: \\n\")\n",
    "actual_vs_predicted_multiplelogisticreg = pd.concat([y_train, pd.DataFrame(multiple_logisticreg_predictions_train, columns=['Predicted'])], axis=1)\n",
    "print(actual_vs_predicted_multiplelogisticreg.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ignored-yacht",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some more evaluation metrics.\n",
    "print(\"==================== Train Data =======================\")\n",
    "print(\"Accuracy: \", metrics.accuracy_score(y_train, multiple_logisticreg_predictions_train))\n",
    "print(\"Confusion matrix: \\n\", metrics.confusion_matrix(y_train, multiple_logisticreg_predictions_train))\n",
    "print(\"Classification report:\\n \", metrics.classification_report(y_train, multiple_logisticreg_predictions_train))\n",
    "print(\"======================================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "owned-deadline",
   "metadata": {},
   "source": [
    "##### Interpretation of results\n",
    "- Accuracy\n",
    "    - This is simply stating how often the model is correct. We have an accuracy of 96.7%\n",
    "- Confusion Matrix - A confusion matrix is a table that is often used to describe the performance of a classification model (or \"classifier\") on a set of test data for which the true values are known.\n",
    "    - [0][0] TRUE NEGATIVE - The number that is predicted 0 that was actually 0\n",
    "    - [0][1] FALSE POSITIVE - The number that is predicted 1 that is actually 0\n",
    "    - [1][0] FALSE NEGATIVE - The number that is predicted 0 that is actually 1\n",
    "    - [1][1] TRUE POSITIVE - The number that is predicted 1 that is actually 1\n",
    "- Precision - How good the model is at predicting the positive class\n",
    "    - What % of the predicted positive are actually positive\n",
    "    - It is the number values correctly predicted positive over the total number of  positive values\n",
    "    - Precision Positive is 0.59\n",
    "    - Precision Negative is 0.97\n",
    "- Recall - The precise definition of recall is the number of true positives divided by the number of true positives plus the number of false negatives. \n",
    "    - What % of the positive values did we predict\n",
    "    - Is the number correctly predicted positive over the total number actual positive\n",
    "    - Recall Positive is 0.31\n",
    "    - Recall Negative is 0.99\n",
    "- F1 Score\n",
    "    - Is an a weighted average of Precision and recall\n",
    "    - F1 Score Positive is 0.40\n",
    "    - F1 Score Negative is 0.98\n",
    "\n",
    "Summary\n",
    "- Theses values seem reasonable. \n",
    "- The model is a much better at predicting the negative class i.e not death.\n",
    "- This makes sense as the model has significantly more negative data to learn from - it is the majority class. \n",
    "- This model achieves better recall values (Recall Positive is 0.31, Recall Negative is 0.99) than the linear regression model (Recall Positive is 0.16, Recall Negative is 1.00).\n",
    "- This may be due to the fact that outliers in this model do not significantly affect the output and that logistic regression models output discrete binary classification numbers which suits the problem of this assignment and not a continous output that results from a linear regression model. \n",
    "- This is still not ideal as the model may underestimate the number of deaths as a result of COVID-19 as a recall value of 0.31 still results in a significant amount of false negatives.\n",
    "- Again, the down side of this is more people who have a risk factor may die as a result of COVID-19 because the model understimated their liklihood of dying due to a COVID-19 related risk.\n",
    "- This would not be an appropriate model to use to determine risk factors of COVID-19 deaths as a result but it is a slightly better model than the linear regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "downtown-atlantic",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the prediction and threshold the value. If >= 0.5 its true\n",
    "multiple_logisticreg_predictions_test = multiple_logisticreg.predict(X_test)\n",
    "\n",
    "print(\"\\nPredictions with multiple linear regression: \\n\")\n",
    "actual_vs_predicted_multiplelogisticreg = pd.concat([y_test, pd.DataFrame(multiple_logisticreg_predictions_test, columns=['Predicted'])], axis=1)\n",
    "print(actual_vs_predicted_multiplelogisticreg.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "preceding-chapter",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some more evaluation metrics.\n",
    "print(\"==================== Test Data =======================\")\n",
    "print(\"Accuracy: \", metrics.accuracy_score(y_test, multiple_logisticreg_predictions_test))\n",
    "print(\"Confusion matrix: \\n\", metrics.confusion_matrix(y_test, multiple_logisticreg_predictions_test))\n",
    "print(\"Classification report:\\n \", metrics.classification_report(y_test, multiple_logisticreg_predictions_test))\n",
    "print(\"==================== Train Data =======================\")\n",
    "print(\"Accuracy: \", metrics.accuracy_score(y_train, multiple_logisticreg_predictions_train))\n",
    "print(\"Confusion matrix: \\n\", metrics.confusion_matrix(y_train, multiple_logisticreg_predictions_train))\n",
    "print(\"Classification report:\\n \", metrics.classification_report(y_train, multiple_logisticreg_predictions_train))\n",
    "print(\"======================================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "official-director",
   "metadata": {},
   "source": [
    "**Interpretation of test results and comparison with training results**\n",
    "- Accuracy\n",
    "    - The accuracy of the test data is marginally higher 96.8% vs 96.7%. \n",
    "- Precision\n",
    "    - The precision score for predicting the positive case did not change it was 59% in both cases.\n",
    "    - The precision score for predicting the negative case has increased from 97% to 98%.\n",
    "- Recall\n",
    "    - The recall score for predicting the positive case has increased from 31% to 35%.\n",
    "    - The recall score for predicting the negative case did not change, it was 99% in both cases.\n",
    "- F1\n",
    "    - The f1 score for predicting the positive case has increased from 40% to 44%.\n",
    "    - The f1 score for predicting the negative case did not change, it was 98% in both cases.\n",
    "\n",
    "**Summary**\n",
    "- These values are a little higher than expected as we are now testing the model prediction on data it has not seen before\n",
    "- This is a good sign that the model is generalising\n",
    "- More comparisons need to be made and this is where cross validation steps in"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "determined-malaysia",
   "metadata": {},
   "source": [
    "## Part 3 3.4 Cross validation\n",
    "We will first create function to perform 10 fold cross validation and store results into dataframe\n",
    "- This will be used to simplify further analysis the dataset, looking at accuracy, precision, recall, f1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "white-murder",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_val_LogReg_DF(X,y):\n",
    "    \"\"\"Function to perform cross validation and store results \n",
    "    in dataframe. Cross validation looks at accuracy, precision, \n",
    "    recall, f1. Returns a dataframe with results\"\"\"\n",
    "\n",
    "    logRegResults = {}\n",
    "    # metrics to test against\n",
    "    test_metrics = ['accuracy','precision','recall', 'f1']\n",
    "\n",
    "    for metric in test_metrics:\n",
    "        # generate test results\n",
    "        result = cross_val_score(LogisticRegression(), X, y, scoring=metric, cv=10)\n",
    "        # store result in dict\n",
    "        logRegResults[metric] = result.mean()\n",
    "        \n",
    "    # create dataframe with results\n",
    "    LogRegDF = pd.DataFrame.from_dict(logRegResults, orient='index', columns=['Logistic_Regression'])\n",
    "    \n",
    "    return LogRegDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "engaged-worship",
   "metadata": {},
   "outputs": [],
   "source": [
    "logRegDF = cross_val_LogReg_DF(X,y)\n",
    "print(f\"Mean results from 10 fold cross validation are:\")\n",
    "logRegDF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "short-culture",
   "metadata": {},
   "source": [
    "## Part 4 Random Forest Model\n",
    "\n",
    "### 4.1 Train a random forest model using only the descriptive features selected from part 1 above \n",
    "- A random forest model will now be trained on our test data. \n",
    "- We use the RandomForestClassifier() function. \n",
    "- The random state will be set to 1 to allow the results to be repeated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "induced-accreditation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train RF with 100 trees\n",
    "rfc = RandomForestClassifier(n_estimators=100, max_features='auto', oob_score=True, random_state=1)\n",
    "rfc.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hawaiian-strategy",
   "metadata": {},
   "source": [
    "### 4.2 Understanding the Random Forest model\n",
    "\n",
    "- A random forest is made up of a collection of decision trees, in order to understand random forests, we must understand decision trees\n",
    "- Decision tree\n",
    "    - A decision tree is a set of if-then-else rules based on splitting the data based on specific features\n",
    "    - Feature split is based on information value \n",
    "    - The tree will split based on the feature that gives the highest information value\n",
    "    - The feature with the highest information value will sit at the root of each decision tree\n",
    "    - Usually the higher the value in information, the higher the root is.\n",
    "    - Each decision tree will be a single prediction\n",
    "- Random Forest \n",
    "    - The random forest is made up of decision trees\n",
    "    - The output of the random forest is based on all the decision trees combined\n",
    "    - This gives reliable results but can be hard to interpret\n",
    "   \n",
    "    \n",
    "** Got information from lab solution example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "assisted-spell",
   "metadata": {},
   "outputs": [],
   "source": [
    "dtc4 = DecisionTreeClassifier(max_depth=4, random_state=1)\n",
    "dtc10 = DecisionTreeClassifier(max_depth=10, random_state=1)\n",
    "dtc4.fit(X_train, y_train)\n",
    "dtc10.fit(X_train, y_train)\n",
    "print(\"Max depth 4: \\n\",dtc4)\n",
    "print(\"Max depth 10: \\n\",dtc10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acute-usage",
   "metadata": {},
   "outputs": [],
   "source": [
    "import graphviz\n",
    "from graphviz import Source\n",
    "# create a Graphviz png\n",
    "with open(\"DecisionTree4.dot\", 'w') as f1:\n",
    "    f1 = export_graphviz(dtc4, out_file=f1, feature_names=X_train.columns)\n",
    "with open(\"DecisionTree10.dot\", 'w') as f2:\n",
    "    f2 = export_graphviz(dtc10, out_file=f2, feature_names=X_train.columns)\n",
    "!dot -Tpng DecisionTree4.dot -o DecisionTree4.png\n",
    "!dot -Tpng DecisionTree10.dot -o DecisionTree10.png"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "african-worse",
   "metadata": {},
   "outputs": [],
   "source": [
    "importance = pd.DataFrame({'feature': X_train.columns, 'importance':rfc.feature_importances_})\n",
    "importance.sort_values('importance', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unauthorized-possible",
   "metadata": {},
   "source": [
    "- This proves that age_group 80+ has the highest information value in terms of prediction of risk of death, this may be sue to the fact that there is a strong relationship between age groups and death, with ~ 35% of 80+ year olds COVID-19 patients dying.\n",
    "\n",
    "- Second highest important feature is hospitalisations Yes, due to the significant relationship between patients who were hospitalised and patients who died.\n",
    "\n",
    "- These features that rank the highest are also in line with what was seen on the correlation co-efficients in the logistic regression model. Strong relationships were seen between age group 80+ years, hosp_yn_yes and icu_yn_yes and the target feature. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "short-night",
   "metadata": {},
   "source": [
    "## Part 4 4.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fallen-township",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicted class labels for all examples, \n",
    "# using the trained model, on in-sample data (same sample used for training and test)\n",
    "rfc_predictions_train = rfc.predict(X_train)\n",
    "df_true_vs_rfc_predicted = pd.DataFrame({'ActualClass': y_train, 'PredictedClass': rfc_predictions_train})\n",
    "df_true_vs_rfc_predicted.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "assisted-disaster",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"==================== Train Data =======================\")\n",
    "print(\"Accuracy: \", metrics.accuracy_score(y_train, rfc_predictions_train))\n",
    "print(\"Confusion matrix: \\n\", metrics.confusion_matrix(y_train, rfc_predictions_train))\n",
    "print(\"Classification report:\\n \", metrics.classification_report(y_train, rfc_predictions_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spanish-geography",
   "metadata": {},
   "source": [
    "##### Interpretation of results\n",
    "- Accuracy\n",
    "    - This is simply stating how often the model is correct. We have an accuracy of 96.7%\n",
    "- Confusion Matrix - A confusion matrix is a table that is often used to describe the performance of a classification model (or \"classifier\") on a set of test data for which the true values are known.\n",
    "    - [0][0] TRUE NEGATIVE - The number that is predicted 0 that was actually 0\n",
    "    - [0][1] FALSE POSITIVE - The number that is predicted 1 that is actually 0\n",
    "    - [1][0] FALSE NEGATIVE - The number that is predicted 0 that is actually 1\n",
    "    - [1][1] TRUE POSITIVE - The number that is predicted 1 that is actually 1\n",
    "- Precision - How good the model is at predicting the positive class\n",
    "    - What % of the predicted positive are actually positive\n",
    "    - It is the number values correctly predicted positive over the total number of  positive values\n",
    "    - Precision Positive is 0.73\n",
    "    - Precision Negative is 0.97\n",
    "- Recall - The precise definition of recall is the number of true positives divided by the number of true positives plus the number of false negatives. \n",
    "    - What % of the positive values did we predict\n",
    "    - Is the number correctly predicted positive over the total number actual positive\n",
    "    - Recall Positive is 0.24\n",
    "    - Recall Negative is 1.0\n",
    "- F1 Score\n",
    "    - Is an a weighted average of Precision and recall\n",
    "    - F1 Score Positive is 0.36\n",
    "    - F1 Score Negative is 0.98\n",
    "\n",
    "Summary\n",
    "- Theses values seem reasonable. \n",
    "- Again, siilar to the other models this model is a much better at predicting the negative class i.e not death.\n",
    "- This makes sense as the model has significantly more negative data to learn from - it is the majority class. \n",
    "- This model achieves better recall values than the linear regression model but worse recall values than logistic regression model. \n",
    "- This model is still not ideal as the model may underestimate the number of deaths as a result of COVID-19 as a recall value of 0.24 still results in a significant amount of false negatives.\n",
    "- Again, the down side of this is more people who have a risk factor may die as a result of COVID-19 because the model understimated their liklihood of dying due to a COVID-19 related risk.\n",
    "- This would not be an appropriate model to use to determine risk factors of COVID-19 deaths as a result but it is a slightly better model than the linear regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "moderate-depth",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicted class labels for all examples, \n",
    "# using the trained model, on in-sample data (same sample used for training and test)\n",
    "rfc_predictions_test = rfc.predict(X_test)\n",
    "df_true_vs_rfc_predicted_test = pd.DataFrame({'ActualClass': y_test, 'PredictedClass': rfc_predictions_test})\n",
    "df_true_vs_rfc_predicted_test.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "complimentary-marketing",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"==================== Test Data =======================\")\n",
    "print(\"Accuracy: \", metrics.accuracy_score(y_test, rfc_predictions_test))\n",
    "print(\"Confusion matrix: \\n\", metrics.confusion_matrix(y_test, rfc_predictions_test))\n",
    "print(\"Classification report:\\n \", metrics.classification_report(y_test, rfc_predictions_test))\n",
    "print(\"==================== Train Data =======================\")\n",
    "print(\"Accuracy: \", metrics.accuracy_score(y_train, rfc_predictions_train))\n",
    "print(\"Confusion matrix: \\n\", metrics.confusion_matrix(y_train, rfc_predictions_train))\n",
    "print(\"Classification report:\\n \", metrics.classification_report(y_train, rfc_predictions_train))\n",
    "print(\"======================================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "answering-garage",
   "metadata": {},
   "source": [
    "**Interpretation of test results and comparison with training results**\n",
    "- Accuracy\n",
    "    - The accuracy of the test data is marginally lower 96.83% vs 97%. \n",
    "- Precision\n",
    "    - The precision score for predicting the negative case did not change it was 97% in both cases.\n",
    "    - The precision score for predicting the positive case has decreased in the test case from 73% to 46%.\n",
    "- Recall\n",
    "    - The recall score for predicting the positive case has decreased in the test case from 24% to 13%.\n",
    "    - The recall score for predicting the negative case, it was 100% in the train case and 99% in the test case.\n",
    "- F1\n",
    "    - The f1 score for predicting the positive case has dercreased in the test case from 36% to 20%.\n",
    "    - The f1 score for predicting the negative case did not change, it was 98% in both cases.\n",
    "\n",
    "**Summary**\n",
    "- These values are a little lower than expected as we are now testing the model prediction on data it has not seen before\n",
    "- This is a sign that the model is not generalising \n",
    "- More comparisons need to be made and this is where cross validation steps in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "induced-slope",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_val_RandomForest_DF(X,y, depth=None, estimators=100):\n",
    "    \"\"\"Function to perform cross validation and store results \n",
    "    in dataframe. Cross validation looks at accuracy, precision, \n",
    "    recall, f1. Returns a dataframe with results\"\"\"\n",
    "\n",
    "    # store results in dict\n",
    "    RandomForestResults = {}\n",
    "    # metrics to test against\n",
    "    test_metrics = ['accuracy','precision','recall', 'f1']\n",
    "\n",
    "    for metric in test_metrics:\n",
    "        # generate test results\n",
    "        result = cross_val_score(RandomForestClassifier(n_estimators=estimators, max_features='auto', oob_score=True, random_state=1, max_depth=depth), X, y, scoring=metric, cv=10)\n",
    "        # store result in dict\n",
    "        RandomForestResults[metric] = result.mean()\n",
    "    \n",
    "    # create dataframe with results\n",
    "    RandomForestDF = pd.DataFrame.from_dict(RandomForestResults, orient='index', columns=['Random_Forests'])\n",
    "\n",
    "    return RandomForestDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acceptable-girlfriend",
   "metadata": {},
   "outputs": [],
   "source": [
    "RandomForestDF = cross_val_RandomForest_DF(X,y)\n",
    "print(f\"Mean results from 10 fold cross validation are:\")\n",
    "RandomForestDF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "innocent-highland",
   "metadata": {},
   "source": [
    "Out of bag accuracy\n",
    "- This is the out of bag error estimate \n",
    "- It is an internal error estimate of a random forest as it is being constructed.\n",
    "- It is used as an additional measure and should be line with the cross validation results\n",
    "- This is generally expected to be the lowest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pending-chuck",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the out-of-bag classification accuracy\n",
    "rfc.oob_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "disciplinary-shakespeare",
   "metadata": {},
   "source": [
    "This is similar to the cross validation accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "severe-night",
   "metadata": {},
   "outputs": [],
   "source": [
    "ResultsDF = pd.concat([linRegDF, logRegDF, RandomForestDF], axis=1)\n",
    "ResultsDF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "shaped-precipitation",
   "metadata": {},
   "source": [
    "## Part 5 Improving Predictive Models.\n",
    "\n",
    "### 5.1 Which model of the ones trained above performs better at predicting the target feature? \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "intense-sending",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Firstly test a simple model to see if any of the models above are better than a simple model\n",
    "good_count = y_test[y_test == 1].count()\n",
    "bad_count = y_test[y_test == 0].count()\n",
    "total = good_count+ bad_count\n",
    "print(f'From original dataset: \\t\\tCount {total}')\n",
    "print(f'Total number deaths:\\t {good_count}\\t{round(good_count/len(y_test)*100,2)}%')\n",
    "print(f'Total number non-deaths:\\t {bad_count}\\t{round(bad_count/len(y_test)*100,2)}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "greek-weekly",
   "metadata": {},
   "source": [
    "- shows the majority class is non-deaths\n",
    "- will now generate scores for predicting the majority class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "signal-ukraine",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate array of 2860 zeros (length of test dataset)\n",
    "majority = np.zeros(2860)\n",
    "# create dataframe \n",
    "df_majorityClass = pd.DataFrame(majority, columns=['prediction'])\n",
    "# calculate scores of simple predictin vs actual\n",
    "print(\"Accuracy: \", metrics.accuracy_score(y_test, df_majorityClass))\n",
    "print(\"Confusion matrix: \\n\", metrics.confusion_matrix(y_test, df_majorityClass))\n",
    "print(\"Classification report:\\n \", metrics.classification_report(y_test, df_majorityClass))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "polish-thousand",
   "metadata": {},
   "source": [
    "- For predicting good outcome the simple model fails completely as expected\n",
    "- For predicting the bad outcome the simple model fares much better but still worse than all the other models outlined above\n",
    "    - The precision is the same as the accuracy in this case - i.e. how good is it at predicting 0\n",
    "    - The recall is 100% as all of the actual 0 values were predicted 0\n",
    "    - The f1 is between the precision and recall as it is a weighted average of the two\n",
    "\n",
    "The best model at predicting the target feature is a Logistic Regression model as it has the highest accuracy, precision and recall values in comparison to linear regression and random forest models. This model is better than a simple model that always predicts the majority calss as the precision positive value is 59% meaning it correctly identifies true positives 59% of the time and the precision negative is 97%, meaning the model correctly identifies true negatives 97% of the time. This would be better than a simple model due to the fact that the simple model has a much lower precision positive value than this logisitic regression model because it would identifies true positives as negatives due to fact that it always predict the majority class which is death no. This would be a dangerous and useless model to use on such critical information.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "animal-nothing",
   "metadata": {},
   "source": [
    "## Part 5 **5.2)** \n",
    "\n",
    "However with the logistic regression model, the recall value is still too low to be able to use this model on real world data, especially dealing with data that is critical to the life of a person. As previously stated this model is much better at predicting the negative class i.e not death. This makes sense as the model has significantly more negative data to learn from - it is the majority class. The problem is that this model is not ideal as it does not suit the need to determine the risk of death in COVID-19 positive patients. This represents an imbalanced classification problem: we have two classes we need to identify — death and not death — with one category representing the overwhelming majority of the data points i.e not death. This is not ideal as the model may underestimate the number of deaths as a result of COVID-19. \n",
    "\n",
    "- One way to try and fix this imbalance is to oversample the data to try and balance the classes. This can be done with Synthetic Minority Oversampling Technique. This approach addresses imbalanced datasets by oversampling the minority class i.e. death yes. The simplest approach involves duplicating examples in the minority class, although these examples don’t add any new information to the model.  Instead, new examples can be synthesized from the existing examples.\n",
    "\n",
    "- Usually when trying to improve a model, one needs to understand the problem. In this case, we want a model that has as low as possible number of false negatives. Meaning, we don't want a model that predicts lots people who won't die of COVID-19 when in reality they will due to the risk factors associated with death. So for this model we want a high recall value. This model at present underestimates risk of death we don't want this for our problem.\n",
    "\n",
    "- Maximizing precision will minimize the number false positives, whereas maximizing the recall will minimize the number of false negatives.\n",
    "\n",
    "    - Precision: Appropriate when minimizing false positives is the focus.\n",
    "    - Recall: Appropriate when minimizing false negatives is the focus.\n",
    "\n",
    "- This oversampling approach will hopefully improve the recall value. \n",
    "\n",
    "\n",
    "### Use Oversampling to improve the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "charged-bermuda",
   "metadata": {},
   "outputs": [],
   "source": [
    "import imblearn\n",
    "print(imblearn.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "owned-python",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get values of death_yn\n",
    "deaths = df['death_yn']\n",
    "deaths.value_counts()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sought-bermuda",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot number of yes and no deaths count to show the imbalance in data.\n",
    "ax = sns.countplot(x=\"death_yn\", data=df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "further-greenhouse",
   "metadata": {},
   "source": [
    "It is evident that the model is imbalanced as the class death_yn 'no'or '0' has significantly more values than death_yn 'yes' or '1'. This means the model will be heavily skewed in favour of the majority class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "furnished-gambling",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "#Create an oversampled training data\n",
    "smote = SMOTE(random_state = 101)\n",
    "X_oversample, y_oversample = smote.fit_resample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "offensive-mandate",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training with oversampled data\n",
    "# Importing the splitter, classification model, and the metric\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "classifier_o = LogisticRegression()\n",
    "classifier_o.fit(X_oversample, y_oversample)\n",
    "print(\"==================== Balanced Train Data =======================\")\n",
    "print(metrics.classification_report(y_train, classifier_o.predict(X_train)))\n",
    "print(\"Accuracy: \", metrics.accuracy_score(y_train, classifier_o.predict(X_train)))\n",
    "print(\"Confusion matrix: \\n\", metrics.confusion_matrix(y_train, classifier_o.predict(X_train)))\n",
    "\n",
    "# Some more evaluation metrics.\n",
    "print(\"==================== Imbalanced Train Data =======================\")\n",
    "print(\"Accuracy: \", metrics.accuracy_score(y_train, multiple_logisticreg_predictions_train))\n",
    "print(\"Confusion matrix: \\n\", metrics.confusion_matrix(y_train, multiple_logisticreg_predictions_train))\n",
    "print(\"Classification report:\\n \", metrics.classification_report(y_train, multiple_logisticreg_predictions_train))\n",
    "print(\"======================================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acute-pearl",
   "metadata": {},
   "source": [
    "- It is evident from the Balanced data that the positive recall value has improved dramatically from 31% in the imbalanced training data to 93% in the oversampled balanced data, this is a dramatic increase and significantly improves the model. This recall value means that it correctly identifies relevant data i.e true positives meaning it reduces the number of predicted false negatives.\n",
    "\n",
    "- The negative recall value is slightly reduced in the balanced oversampled data 91% in comparison to the imbalanced train set 99%.\n",
    "\n",
    "- The positive precision value has reduced significantly aswell in the balanced train set 28% in comparison to 59% in the imbalanced train set. Precision would be expected to decrease significantly because the output predictions will have more positive predictions in the train set and since our data is so heavily skewed towards a negative outcome it will produce a lot of false positives resulting in a reduced precision value for positives.\n",
    "\n",
    "- In imbalanced datasets, the goal is to improve recall without hurting precision. These goals, however, are often conflicting, since in order to increase the True Positives for the minority class, the number of False Positives is also often increased, resulting in reduced precision.\n",
    "\n",
    "- However, in this case we are more interested in improving recall because a model that overestimates risk of death is better than underestimating risk of death.\n",
    "\n",
    "source: https://machinelearningmastery.com/precision-recall-and-f-measure-for-imbalanced-classification/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "blond-woman",
   "metadata": {},
   "source": [
    "- It is evident from the tables above, that oversampling the data dramatically improved the overall recall and precision values  and also improved the f1 score aswell, F-Measure provides a way to combine both precision and recall into a single measure that captures both properties.\n",
    "\n",
    "- Alone, neither precision or recall tells the whole story. F-measure provides a way to express both precision and recall with a single value, in this case the f1 score has increased from 41% to 44%. \n",
    "\n",
    "- Accuracy has reduced slightly from 97% to 91% in the oversampled train set, this is to be expected as oversampling a train set means more true positives are introduced in the data but this also means more false positives are introduced into the data aswell, resulting in reduced accuracy.\n",
    "\n",
    "- Overall this model implemented with smote could be used on real-life data as recall is 95%, meaning the sensitivity of this model correctly predicts the risk of death in COVID-19 positive cases 95% of the time. I would be confident in using this model on real world data to predict risk of death in COVID-19 positive patients.\n",
    "\n",
    "source: https://machinelearningmastery.com/precision-recall-and-f-measure-for-imbalanced-classification/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "backed-tennis",
   "metadata": {},
   "source": [
    "## Check to see how the model works on test data that the model hasn't seen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "likely-vulnerability",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training with oversampled data\n",
    "# Importing the splitter, classification model, and the metric\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "classifier_o = LogisticRegression()\n",
    "classifier_o.fit(X_oversample_test, y_oversample_test)\n",
    "print(\"==================== Balanced Test Data =======================\")\n",
    "print(metrics.classification_report(y_test, classifier_o.predict(X_test)))\n",
    "print(\"Accuracy: \", metrics.accuracy_score(y_test, classifier_o.predict(X_test)))\n",
    "print(\"Confusion matrix: \\n\", metrics.confusion_matrix(y_test, classifier_o.predict(X_test)))\n",
    "\n",
    "# Some more evaluation metrics.\n",
    "print(\"==================== Imbalanced Test Data =======================\")\n",
    "print(\"Accuracy: \", metrics.accuracy_score(y_test, multiple_logisticreg_predictions_test))\n",
    "print(\"Confusion matrix: \\n\", metrics.confusion_matrix(y_test, multiple_logisticreg_predictions_test))\n",
    "print(\"Classification report:\\n \", metrics.classification_report(y_test, multiple_logisticreg_predictions_test))\n",
    "print(\"======================================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bacterial-dragon",
   "metadata": {},
   "source": [
    "- The improved model using smote shows the values have significantly improved from the imbalanced test data.\n",
    "\n",
    "- accuracy has reduced from 97% to 93%.\n",
    "- precision has increased from 62% to 93%.\n",
    "- recall has increased from 33% to 93%.\n",
    "- f1 score has increased from 42% to 93%.\n",
    "\n",
    "This proves that the model can be used on data it has not seen before and accuracy and recall is 93%.\n",
    "- recall value in test data is slightly lower than in train data, this is to be expected as the model has not seen the new test data, whereas the model was trained on the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "polyphonic-greensboro",
   "metadata": {},
   "source": [
    "## Part 5  **(5.3)**\n",
    "\n",
    "### Test other csv provided using the trained Logistic Regression Model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "processed-judges",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read csv file into a dataframe and it was cleaned prior using the homework1 jupyter notebook, that is why it has a new name.\n",
    "df_new = pd.read_csv('covid19-cdc-13336431-cleaned_data_Final_testCase.csv', keep_default_na=True, sep=',\\s+', delimiter=',', skipinitialspace=True)\n",
    "df_new.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "structural-minister",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "billion-vector",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "extensive-repeat",
   "metadata": {},
   "source": [
    "#### Convert datatypes for plotting later\n",
    "We will now review the datatypes and convert if needed. This will help avoid plotting errors later in the notebook\n",
    "- The target feature \"death_yn\" is type object, with values \"Yes\" & \"No\". These will be mapped 'yes': 1, \"no\": 0 and stored as \"int64\"\n",
    "- Categorical features will stay as category datatypes\n",
    "- Some Continuous features are datetime64 type\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "antique-natural",
   "metadata": {},
   "source": [
    "#### Setup Column types\n",
    "We will now setup the continuous, categorical, target features\n",
    "\n",
    "##### Select all categorical columns and convert to categorical type\n",
    " - This will be needed later when it will be required to convert categorical features into dummy features for modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "radical-recording",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert riskperformance to 0,1\n",
    "df_new['death_yn'] = df_new['death_yn'].map({'Yes': 1, \"No\": 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "split-pendant",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert date time features to appropriate date time data types\n",
    "df_new['cdc_case_earliest_dt'] = df_new['cdc_case_earliest_dt'].astype('datetime64[ns]')\n",
    "df_new.dtypes\n",
    "\n",
    "#Select all columns of type 'object'\n",
    "object_columns = df_new.select_dtypes(['object']).columns\n",
    "object_columns\n",
    "#Convert selected columns to type 'category'\n",
    "for column in object_columns:\n",
    "    df_new[column] = df_new[column].astype('category')\n",
    "df_new.dtypes \n",
    "continous_columns = df_new.select_dtypes(['datetime64[ns]']).columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "blank-estate",
   "metadata": {},
   "source": [
    "##### Finally set the target feature \"death_yn\" to int64 \n",
    "- death_yn is a categorical feature but it is also the target feature\n",
    "- To allow continuous features to plot against the target, it will need to be int64 type\n",
    "- We will also remove it from the list of categorical columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "heavy-circulation",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new['death_yn'] = df_new['death_yn'].astype('int64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stuck-revelation",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exotic-conditions",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = pd.DataFrame(df_new[\"death_yn\"])\n",
    "X = df_new.drop([\"death_yn\"],1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "favorite-restoration",
   "metadata": {},
   "outputs": [],
   "source": [
    "low_gain_features = ['current_status', 'sex', 'Race', 'Ethnicity']\n",
    "low_correlation_features = ['cdc_case_earliest_dt']\n",
    "# drop all low value features\n",
    "low_value_features = list(set(low_gain_features + low_correlation_features))\n",
    "print(low_value_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pointed-peace",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop all low value features\n",
    "# before dropping make copy of original\n",
    "df_rev1_new = df_new.copy()\n",
    "# drop low value features\n",
    "df_rev1_new.drop(low_value_features, 1, inplace=True)\n",
    "print('\\nRemaining columns:', df_rev1_new.columns)\n",
    "print('\\nNew shape:', df_rev1_new.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "generic-injury",
   "metadata": {},
   "source": [
    "## Prepare dataset for modeling\n",
    "Now we have picked our descriptive features for the whole dataset, a number of additional steps will need to be taken to prepare the dataset for modeling \n",
    "- We will now convert the categorical variables into dummies variable to allow modeling\n",
    "- We will then set up the train test split again based on the dataset with the dummies included"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "distinct-galaxy",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rev1_new = pd.get_dummies(df_rev1_new, columns=['age_group','hosp_yn','icu_yn','medcond_yn'], drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fundamental-discrimination",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_columns = df_rev1_new.select_dtypes(include=['uint8']).columns.tolist()\n",
    "\n",
    "\n",
    "categorical_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "scientific-termination",
   "metadata": {},
   "outputs": [],
   "source": [
    "# y is the target\n",
    "y = df_rev1_new[\"death_yn\"]\n",
    "\n",
    "\n",
    "# X is everything else\n",
    "X = df_rev1_new.drop([\"death_yn\"],1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fifteen-australia",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some more evaluation metrics.\n",
    "print(\"==================== Provided New CSV Imbalanced Data when SMOTE is not applied =======================\")\n",
    "print(\"Accuracy: \", metrics.accuracy_score(y, multiple_logisticreg.predict(X)))\n",
    "print(\"Confusion matrix: \\n\", metrics.confusion_matrix(y, multiple_logisticreg.predict(X)))\n",
    "print(\"Classification report:\\n \", metrics.classification_report(y, multiple_logisticreg.predict(X)))\n",
    "\n",
    "print(\"==================== Provided New CSV Balanced Data when SMOTE is applied=======================\")\n",
    "print(metrics.classification_report(y, classifier_o.predict(X)))\n",
    "print(\"Accuracy: \", metrics.accuracy_score(y, classifier_o.predict(X)))\n",
    "print(\"Confusion matrix: \\n\", metrics.confusion_matrix(y, classifier_o.predict(X)))\n",
    "\n",
    "print(\"==================== Balanced Test Data on csv from Homework 1 =======================\")\n",
    "print(metrics.classification_report(y_test, classifier_o.predict(X_test)))\n",
    "print(\"Accuracy: \", metrics.accuracy_score(y_test, classifier_o.predict(X_test)))\n",
    "print(\"Confusion matrix: \\n\", metrics.confusion_matrix(y_test, classifier_o.predict(X_test)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "protective-liver",
   "metadata": {},
   "source": [
    "## Summary of Findings:\n",
    "\n",
    "- Applying the new csv to the trained model that had implemented SMOTE shows similar results to the results when the test data from the cleaned dataframe csv from homework1 was used on the model to predict death in COVID-19 positive patients.\n",
    "\n",
    "- Positive Precision value is low, as is expected when increased numbers of false positives are in the dataset.\n",
    "- Recall value is 64%, which is not as high as 84% seen in the test data. This may be due to the fact that the test data is 30% of the actual dataset in comparison to the whole dataset from the new csv which was used in the model.\n",
    "- Accuracy is similar 93% in both homework1 csv test data and provided new csv data.\n",
    "\n",
    "Conclusion:\n",
    "\n",
    "- 3 predictive models have been evaluated in this notebook, with all models performing very similarly\n",
    "- The logistic and random forest models perform almost the same +/-10% and both could be used. \n",
    "- The logistic model shows better results in accuracy and recall in comparison to the other two models.\n",
    "- 64% recall on new data using the best logistic regression trained model is not a bad result. However, it is not a high enough value to make me believe that this could be used on real world data with confidence, due to the fact that the output is of critical importance to the lives of COVID-19 positive patients. \n",
    "\n",
    "Recommendations:\n",
    "\n",
    "- As a result, I believe more data is needed to train the model and significantly more data on death_yn yes values, the limited number of the minority class makes it difficult to predict the risk of death confidently.\n",
    "- Possibly more stratified information on certain features such as exact medical conditions may give more insight into risk factors associated with death.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "understanding-mobile",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "relevant-latter",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
